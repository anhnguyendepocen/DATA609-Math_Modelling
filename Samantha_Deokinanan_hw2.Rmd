---
title: 'CUNY MSDS Data 609'
author: "Samantha Deokinanan"
date: "22nd February, 2020"
output:
  html_document: 
    df_print: default
    self_contained: no
    toc: yes
    toc_depth: 4
---

***
### Homework 2.1  

***

#### Problem #2a, 121

For each of the following data sets, formulate the mathematical model that minimizes the largest deviation between the data and the line $y = ax + b$. If a computer is available, solve for the estimates of a and b.

Solving the model by hand is $y = 0.605x + 2.04$, and solving using lm(), the model is $y = 0.56x + 2.21$.

```{r}
x = c(1.0, 2.3, 3.7, 4.2, 6.1, 7.0)
y = c(3.6, 3.0, 3.2, 5.1, 5.3, 6.8)

a = (y[5]-y[2]) / (x[5]-x[2]); a

b = mean(y - a*x); b

# Or easily using lm()
summary(lm(y~x))
```

***

#### Problem #7, 136

a. In the following data, W represents the weight of a fish (bass) and *l* represents its length. Fit the model $W = kl^3$ to the data using the least-squares criterion.

```{r}
l = c(14.5, 12.5, 17.25, 14.5, 12.625, 17.75, 14.125, 12.625)
w = c(27, 17, 41, 26, 17, 49, 23, 16)

k = sum(l^3*w)/sum(l^(2*3)); k
```

Thus, the model is $W = 0.0084l^3$.

b. In the following data, g represents the girth of a fish. Fit the model $W = klg^2$ to the data using the least-squares criterion.

```{r}
g = c(9.75, 8.375, 11.0, 9.75, 8.5, 12.5, 9.0, 8.5)

k = sum(w*(l*g^2))/sum((l*g^2)^2); k
```

Thus, the model is $W = 0.0186lg^2$.

c. Which of the two models fits the data better? Justify fully. Which model do you prefer? Why?

```{r}
model_1 = 0.0084*l^3
model_2 = 0.0186*l*g^2

data.frame(model_1_errors = (model_1 - w), model_2_errors = (model_2 - w))

# root mean squared error
caret::RMSE(model_1, w)
caret::RMSE(model_2, w)

# Plot of the fit
library(ggplot2)
ggplot(data.frame(l,w), aes(x = l, y = w)) + geom_point() + geom_line(aes(x = l, y = model_1), color = "red") + labs(x = "Length (in)", y = "Weight (oz)", title = "Model #1")

ggplot(data.frame(l,w,g), aes(x = l*g^2, y = w)) + geom_point() + geom_line(aes(x = l*g^2, y = model_2), color = "red") + labs(x = "Length (in)", y = "Weight (oz)", title = "Model #2")
```

Model 1 is better. We see that the root mean squared error is smaller of the two which mean that the standard deviation of the model prediction errors is less, and a smaller value would indicate a better model performance.

***

#### Problem #11, 169

Construct a scatterplot of the given data. Is there a trend in the data? Are any of the data points outliers? Construct a divided difference table. Is smoothing with a low-order polynomial appropriate? If so, choose an appropriate polynomial and fit using the least-squares criterion of best fit. Analyze the goodness of fit by examining appropriate indicators and graphing the model, the data points, and the deviations.

```{r}
Length = c(12.5, 12.625, 14.125, 14.5, 17.25, 17.75)
Weight = c(17, 16.5, 23, 26.5, 41, 49)

ggplot(data.frame(Length, Weight), aes(x = Length, y = Weight)) + geom_point() + geom_smooth(method = 'lm', se = TRUE)
``` 

Based on the graph and limited number of points, I wouldn't consider any of them as extreme outliers since they all fall within the confidence interval of a linear tread fit.

```{r}
dd = data.frame(Length, Weight)
dd$delta_y1 = c(NA, diff(dd$Weight)/diff(dd$Length))
dd$delta_y2 = c(NA, NA, na.omit(diff(dd$delta_y1))/diff(dd$Length, lag = 2))
dd$delta_y3 = c(NA, NA, NA, na.omit(diff(dd$delta_y2))/diff(dd$Length, lag = 3))
dd$delta_y4 = c(NA, NA, NA, NA, na.omit(diff(dd$delta_y3))/diff(dd$Length, lag = 4))
dd$delta_y5 = c(NA, NA, NA, NA, NA, na.omit(diff(dd$delta_y4))/diff(dd$Length, lag = 5))  

dd
```

An examination of the divided difference table reveals that the first divided differences that negative values are already present, which may indicate the presence of measurement error or variations in the data that will not be captured with low-order polynomials. Negative signs will also have a detrimental effect on the differences in the remaining columns. Besides, the differences become small by the forth step, thus, the use of a 4th-order model might be sufficient.

```{r}
# 2nd-order model
second = lm(Weight ~ poly(Length,2)); second
ggplot(data.frame(Length, Weight), aes(x = Length, y = Weight)) + geom_point() + geom_line(aes(x = Length, y = second$fitted.values), color = "red") + labs(x = "Length (in)", y = "Weight (oz)", title = "2nd order model")

# 3rd-order model
third = lm(Weight ~ poly(Length,3)); third
ggplot(data.frame(Length, Weight), aes(x = Length, y = Weight)) + geom_point() + geom_line(aes(x = Length, y = third$fitted.values), color = "red") + labs(x = "Length (in)", y = "Weight (oz)", title = "3rd order model")

# 4th-order model
forth = lm(Weight ~ poly(Length,4)); forth
ggplot(data.frame(Length, Weight), aes(x = Length, y = Weight)) + geom_point() + geom_line(aes(x = Length, y = forth$fitted.values), color = "red") + labs(x = "Length (in)", y = "Weight (oz)", title = "4th order model")

data.frame(RMSE_2nd = caret::RMSE(second$fitted.values, Weight), 
           RMSE_3rd = caret::RMSE(third$fitted.values, Weight), 
           RMSE_4th = caret::RMSE(forth$fitted.values, Weight))
```
    
Firtly, we see the models for each respected regression being:

2nd order : $Weight = 28.83 + 29.43 \quad Length + 3.11 \quad Length^2$    
3rd order : $Weight = 28.83 + 29.43 \quad Length + 3.11 \quad Length^2 + 2.38 \quad Length^3$  
4th order : $Weight = 28.83 + 29.43 \quad Length + 3.11 \quad Length^2 + 2.38 \quad Length^3 + 2.09 \quad Length^4$  

And from the graphics, it is clear that the model that fits the data well is the 4th-order model. This model further resulted in a root mean squared error of 0.13, the smallest of the three models. In all, a 4th order model is the best fit for this data.

***
### Homework #2.2 

***  
#### Problem #1, pg 194  

Use the middle-square method to generate:  
a. 10 random numbers using $x_0 = 1009$.  
b. 20 random numbers using $x_0 = 653217$.  
c. 15 random numbers using $x_0 = 3043$.  
d. Comment about the results of each sequence. Was there cycling? Did each sequence degenerate rapidly?

```{r}
mid.square = function(x0, n){
  ran.value = data.frame(x0)
  l = nchar(as.character(x0))
  for (i in c(1:n)){
    temp = as.character(ran.value[i,]^2)
    if (nchar(temp) < l*2){
      zeros = paste(rep("0", l*2 - nchar(temp)), collapse = "")
      temp = paste0(zeros, temp)
    }
    ran.value[i+1,] = as.integer(substr(temp, l-((l/2)-1), l+(l/2)))
    if (nchar(ran.value[i+1,]) == 1){
      return(ran.value[-c(1),])
    }
  }
  return(ran.value[-c(1),])
}

# Random numbers generated using middle square method
mid.square(1009,10)
mid.square(653217, 20)
mid.square(3043,15)
```

***  
#### Problem #3, pg 211  

In many situations, the time T between deliveries and the order quantity Q is not fixed. Instead, an order is placed for a specific amount of gasoline. Depending on how many orders are placed in a given time interval, the time to fill an order varies. You have no reason to believe that the performance of the delivery operation will change. Therefore, you have examined records for the past 100 deliveries and found the following lag times, or extra days, required to fill your order:

|Lag time (in days) | Number of occurrences|  
|-------------------|----------------------|  
|        2          |          10          |  
|        3          |          25          |  
|        4          |          30          |  
|        5          |          20          |  
|        6          |          13          |  
|        7          |           2          |  
| Total:            |         100          |  

Construct a Monte Carlo simulation for the lag time submodel. If you have a handheld calculator or computer available, test your submodel by running 1000 trials and comparing the number of occurrences of the various lag times with the historical data.


```{r}
lag.times = c(2:7)
lag.prob = c(0.1, 0.25, 0.3, 0.2, 0.13, 0.02)

trials = 1000

lag.simulation = sample(x = lag.times, size = trials, prob = lag.prob, replace = TRUE)
lag.results = data.frame(table(lag.simulation))
lag.results = cbind("lag" = lag.results$lag.simulation, "prob" = lag.prob, "pred" = lag.results$Freq/trials)
lag.results
```

After the trials, we see that the simulation comes very close to the historial data given, thus it is rightfully said that the that the performance of the delivery operation will not change.

